# -*- coding: utf-8 -*-
"""RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mUF21_stNa8VT1s-lojSc0-OcA9nhnbi
"""

!pip install -q --upgrade langchain langchain-openai langchain-core langchain_community docx2txt pypdf langchain_chroma sentence_transformers

import langchain

import os
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = os.getenv("LANGCHAIN_API_KEY")
os.environ["LANGCHAIN_PROJECT"] = "personal"

from langchain_openai import ChatOpenAI
llm = ChatOpenAI(model="gpt-4o-mini")
llm_response = llm.invoke("Tell me a joke")
llm_response

from langchain_core.output_parsers import StrOutputParser
output_parser = StrOutputParser()
output_parser.invoke(llm_response)

from langchain_community.document_loaders import Docx2txtLoader,PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from typing import List
from langchain_core.documents import Document

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    length_function=len,
)

def load_documents(folder_path:str) -> List[Document]:
  documents=[]
  for filename in os.listdir(folder_path):
    file_path = os.path.join(folder_path,filename)
    if filename.endswith('.pdf'):
      loader = PyPDFLoader(file_path)
    elif filename.endswith('.docx'):
      loader = Docx2txtLoader(file_path)
    else:
      continue
    documents.extend(loader.load())
  return documents

folder_path = '/content/sample_data'
documents = load_documents(folder_path)

print(f'Loaded {len(documents)} documents from the folder')
splits = text_splitter.split_documents(documents)
print(f'Split into {len(splits)} chunks')

embdeddings = OpenAIEmbeddings()
document_embeddings = embdeddings.embed_documents([split.page_content for split in splits])

from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings
embedding_function = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")

from langchain_chroma import Chroma
embedding_function = OpenAIEmbeddings()
collection_name = "my_collection"
vectorstore = Chroma.from_documents(collection_name=collection_name,documents=splits,embedding=embedding_function,)

query = " "
search_results = vectorstore.similarity_search(query,k=2)
print(f"Top 2 relevant are \n")
for i ,result in enumerate(search_results,1):
  print(f"Result {i}:")
  print(f"Source: {result.metadata.get('source','Unknown')}")
  print(f'Content: {result.page_content}')

retriever = vectorstore.as_retriever(search_kwargs = {'k':2})
retriever.invoke("How were winters in animal farm")

from langchain_core.prompts import ChatPromptTemplate
template = """
Answer the question based only on the following context:
{context}
Question : {question}

Answer:
"""
prompt = ChatPromptTemplate.from_template(template)

from langchain.schema.runnable import RunnablePassthrough
rag_chain = (
    {"context": retriever , "question":RunnablePassthrough()} | prompt
)

def docs2string(docs):
  return "\n\n".join(doc.page_content for doc in docs)

rag_chain = ({"context":retriever | docs2string,"question":RunnablePassthrough()} | prompt )
rag_chain.invoke("What was the constitution of Animal Farm")

rag_chain = (
             {"context":retriever
             |docs2string,"question":RunnablePassthrough()}
             |prompt
             |llm
             |StrOutputParser()
             )
question = "What was the constitution of Animal Farm"
response = rag_chain.invoke(question)
print(response)